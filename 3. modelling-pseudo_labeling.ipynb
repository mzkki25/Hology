{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tf_keras as tfk\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_keras import layers\n",
    "from tf_keras import backend\n",
    "from tf_keras.preprocessing.image import ImageDataGenerator\n",
    "from tf_keras.preprocessing import image\n",
    "from tf_keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = \"dataset/train_end\"\n",
    "TEST = \"dataset/test_end\"\n",
    "TRAIN_CSV = 'train_benchmark.csv'\n",
    "TEST_CSV = 'test_benchmark.csv'\n",
    "\n",
    "PATCH_SIZE = 4\n",
    "EXPANSION_FACTOR = 2\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 64\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(PATH):\n",
    "    df = pd.read_csv(PATH)\n",
    "    df['id'] = df['id'].astype(str) + '.png'\n",
    "    df['jenis'] = df['jenis'].astype(str)\n",
    "    df['warna'] = df['warna'].astype(str)\n",
    "    return df\n",
    "\n",
    "train_data = load_data(TRAIN_CSV)\n",
    "test_data = load_data(TEST_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mobile-ViT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, filters=16, kernel_size=3, strides=2):\n",
    "    x = layers.Conv2D(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        activation=tfk.activations.swish,\n",
    "        padding=\"same\",\n",
    "    )(x)\n",
    "    return x\n",
    "\n",
    "def correct_pad(inputs, kernel_size):\n",
    "    img_dim = 2 if backend.image_data_format() == \"channels_first\" else 1\n",
    "    input_size = inputs.shape[img_dim : (img_dim + 2)]\n",
    "    \n",
    "    if isinstance(kernel_size, int):\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "        \n",
    "    if input_size[0] is None:\n",
    "        adjust = (1, 1)\n",
    "    else:\n",
    "        adjust = (1 - input_size[0] % 2, 1 - input_size[1] % 2)\n",
    "        \n",
    "    correct = (kernel_size[0] // 2, kernel_size[1] // 2)\n",
    "    return (\n",
    "        (correct[0] - adjust[0], correct[0]),\n",
    "        (correct[1] - adjust[1], correct[1]),\n",
    "    )\n",
    "\n",
    "def inverted_residual_block(x, expanded_channels, output_channels, strides=1):\n",
    "    m = layers.Conv2D(expanded_channels, 1, padding=\"same\", use_bias=False)(x)\n",
    "    m = layers.BatchNormalization()(m)\n",
    "    m = tfk.activations.swish(m)\n",
    "\n",
    "    if strides == 2:\n",
    "        m = layers.ZeroPadding2D(padding=correct_pad(m, 3))(m)\n",
    "        \n",
    "    m = layers.DepthwiseConv2D(3, strides=strides, padding=\"same\" if strides == 1 else \"valid\", use_bias=False)(m)\n",
    "    m = layers.BatchNormalization()(m)\n",
    "    m = tfk.activations.swish(m)\n",
    "    m = layers.Conv2D(output_channels, 1, padding=\"same\", use_bias=False)(m)\n",
    "    m = layers.BatchNormalization()(m)\n",
    "\n",
    "    if tf.equal(x.shape[-1], output_channels) and strides == 1:\n",
    "        return layers.Add()([m, x])\n",
    "    \n",
    "    return m\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tfk.activations.swish)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def transformer_block(x, transformer_layers, projection_dim, num_heads=2):\n",
    "    for _ in range(transformer_layers):\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        x2 = layers.Add()([attention_output, x])\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = mlp(\n",
    "            x3, hidden_units=[x.shape[-1] * 2, x.shape[-1]],\n",
    "            dropout_rate=0.1,\n",
    "        )\n",
    "        x = layers.Add()([x3, x2])\n",
    "\n",
    "    return x\n",
    "\n",
    "def mobilevit_block(x, num_blocks, projection_dim, strides=1):\n",
    "    local_features = conv_block(x, filters=projection_dim, strides=strides)\n",
    "    local_features = conv_block(\n",
    "        local_features, filters=projection_dim, kernel_size=1, strides=strides\n",
    "    )\n",
    "\n",
    "    num_patches = int((local_features.shape[1] * local_features.shape[2]) / PATCH_SIZE)\n",
    "    non_overlapping_patches = layers.Reshape((PATCH_SIZE, num_patches, projection_dim))(\n",
    "        local_features\n",
    "    )\n",
    "    global_features = transformer_block(\n",
    "        non_overlapping_patches, num_blocks, projection_dim\n",
    "    )\n",
    "\n",
    "    folded_feature_map = layers.Reshape((*local_features.shape[1:-1], projection_dim))(\n",
    "        global_features\n",
    "    )\n",
    "\n",
    "    folded_feature_map = conv_block(\n",
    "        folded_feature_map, filters=x.shape[-1], kernel_size=1, strides=strides\n",
    "    )\n",
    "    local_global_features = layers.Concatenate(axis=-1)([x, folded_feature_map])\n",
    "\n",
    "    local_global_features = conv_block(\n",
    "        local_global_features, filters=projection_dim, strides=strides\n",
    "    )\n",
    "\n",
    "    return local_global_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MobileViT(num_classes):\n",
    "    tfk.backend.clear_session()\n",
    "    \n",
    "    inputs = tfk.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n",
    "\n",
    "    \"\"\"Initial conv-stem -> MV2 block.\"\"\"\n",
    "    x = conv_block(x, filters=16)\n",
    "    x = inverted_residual_block(x, expanded_channels=16 * EXPANSION_FACTOR, output_channels=16)\n",
    "\n",
    "    \"\"\"Downsampling with MV2 block.\"\"\"\n",
    "    x = inverted_residual_block(x, expanded_channels=16 * EXPANSION_FACTOR, output_channels=24, strides=2)\n",
    "    x = inverted_residual_block(x, expanded_channels=24 * EXPANSION_FACTOR, output_channels=24)\n",
    "    x = inverted_residual_block(x, expanded_channels=24 * EXPANSION_FACTOR, output_channels=24)\n",
    "\n",
    "    \"\"\"First MV2 -> MobileViT block.\"\"\"\n",
    "    x = inverted_residual_block(x, expanded_channels=24 * EXPANSION_FACTOR, output_channels=48, strides=2)\n",
    "    x = mobilevit_block(x, num_blocks=2, projection_dim=64)\n",
    "\n",
    "    \"\"\"Second MV2 -> MobileViT block.\"\"\"\n",
    "    x = inverted_residual_block(x, expanded_channels=64 * EXPANSION_FACTOR, output_channels=64, strides=2)\n",
    "    x = mobilevit_block(x, num_blocks=4, projection_dim=80)\n",
    "\n",
    "    \"\"\"Third MV2 -> MobileViT block.\"\"\"\n",
    "    x = inverted_residual_block(x, expanded_channels=80 * EXPANSION_FACTOR, output_channels=80, strides=2)\n",
    "    x = mobilevit_block(x, num_blocks=3, projection_dim=96)\n",
    "    x = conv_block(x, filters=320, kernel_size=1, strides=1)\n",
    "\n",
    "    \"\"\"Classification head.\"\"\"\n",
    "    x = layers.GlobalAvgPool2D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return tfk.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(num_classes, train_data, test_data, cw):\n",
    "    mobilevit_xxs = MobileViT(num_classes=num_classes)\n",
    "    \n",
    "    mobilevit_xxs.compile(\n",
    "        optimizer=tfk.optimizers.Adam(learning_rate=0.002), \n",
    "        loss=tfk.losses.CategoricalCrossentropy(label_smoothing=0.1), \n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    history = mobilevit_xxs.fit(\n",
    "        train_data, epochs=50,\n",
    "        validation_data=test_data,\n",
    "        callbacks=[tfk.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "        class_weight=cw\n",
    "    )\n",
    "    \n",
    "    score = mobilevit_xxs.evaluate(test_data)\n",
    "    print(\"Test Loss:\", score[0])\n",
    "    print(\"Test Accuracy:\", score[1])\n",
    "    \n",
    "    return history, mobilevit_xxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Define Evaluate Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    train_accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(12, 10))\n",
    "\n",
    "    ax[0].set_title('Training Accuracy vs. Epochs')\n",
    "    ax[0].plot(train_accuracy, 'o-', label='Train Accuracy')\n",
    "    ax[0].plot(val_accuracy, 'o-', label='Validation Accuracy')\n",
    "    ax[0].set_xlabel('Epochs')\n",
    "    ax[0].set_ylabel('Accuracy')\n",
    "    ax[0].legend(loc='best')\n",
    "\n",
    "    ax[1].set_title('Training/Validation Loss vs. Epochs')\n",
    "    ax[1].plot(train_loss, 'o-', label='Train Loss')\n",
    "    ax[1].plot(val_loss, 'o-', label='Validation Loss')\n",
    "    ax[1].set_xlabel('Epochs')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].legend(loc='best')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model, model_name, generator): \n",
    "    y_pred = np.argmax(model.predict(generator), axis=1).tolist()\n",
    "    y_true = generator.classes\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=generator.class_indices.keys(), yticklabels=generator.class_indices.keys())\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training Jenis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_jenis = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ").flow_from_dataframe(\n",
    "    dataframe=train_data,\n",
    "    directory=TRAIN,  \n",
    "    x_col='id',  \n",
    "    y_col='jenis',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    seed=RANDOM_STATE\n",
    ")\n",
    "\n",
    "test_jenis = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ").flow_from_dataframe(\n",
    "    dataframe=test_data,\n",
    "    directory=TEST,  \n",
    "    x_col='id',  \n",
    "    y_col='jenis',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    seed=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_jenis, mobilevit_jenis = run_experiment(\n",
    "    num_classes=2, \n",
    "    train_data=train_jenis, \n",
    "    test_data=test_jenis,\n",
    "    cw=class_weight.compute_class_weight('balanced', np.unique(train_jenis.classes), train_jenis.classes)\n",
    ")\n",
    "mobilevit_jenis.save('mobilevit_jenis.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_jenis)\n",
    "plot_confusion_matrix(mobilevit_jenis, \"MobileViT Jenis\", test_jenis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training Warna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_warna = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ").flow_from_dataframe(\n",
    "    dataframe=train_data,\n",
    "    directory=TRAIN,  \n",
    "    x_col='id',  \n",
    "    y_col='warna',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    seed=RANDOM_STATE\n",
    ")\n",
    "\n",
    "test_warna = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ").flow_from_dataframe(\n",
    "    dataframe=test_data,\n",
    "    directory=TEST,  \n",
    "    x_col='id',  \n",
    "    y_col='warna',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    seed=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_warna, mobilevit_warna = run_experiment(\n",
    "    num_classes=2, \n",
    "    train_data=train_warna, \n",
    "    test_data=test_warna,\n",
    "    cw=class_weight.compute_class_weight('balanced', np.unique(train_warna.classes), train_warna.classes)\n",
    ")\n",
    "mobilevit_warna.save('mobilevit_warna.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_warna)\n",
    "plot_confusion_matrix(mobilevit_warna, \"MobileViT Warna\", test_warna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array, mode='tf', data_format=None)\n",
    "    return img, img_array\n",
    "\n",
    "def visualize_predictions(data: pd.DataFrame, pathOfImage: str, model_jenis, model_warna, num_images=15):\n",
    "    plt.figure(figsize=(15, 9))\n",
    "\n",
    "    sample_data = data.sample(n=num_images)\n",
    "\n",
    "    for i, row in enumerate(sample_data.itertuples()):\n",
    "        img_path = os.path.join(pathOfImage, str(row.id))\n",
    "        img, img_array = load_and_preprocess_image(img_path)\n",
    "\n",
    "        pred_jenis = np.argmax(model_jenis.predict(img_array), axis=1)[0]\n",
    "        pred_warna = np.argmax(model_warna.predict(img_array), axis=1)[0]\n",
    "\n",
    "        actual_jenis = int(row.jenis)\n",
    "        actual_warna = int(row.warna)\n",
    "\n",
    "        label_color = 'black'\n",
    "        if actual_jenis != pred_jenis or actual_warna != pred_warna:\n",
    "            label_color = 'red'\n",
    "\n",
    "        plt.subplot(3, 5, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(\n",
    "            f\"Warna Aktual = {actual_warna}, Prediksi = {pred_warna}\\n\"\n",
    "            f\"Jenis Aktual = {actual_jenis}, Prediksi = {pred_jenis}\", \n",
    "            color=label_color\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(train_data, TRAIN, mobilevit_jenis, mobilevit_warna)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
